{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.stats import zscore\n",
    "import obspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_versions():\n",
    "    packages = {\n",
    "        'numpy': 'np',\n",
    "        'pandas': 'pd',\n",
    "        'sklearn': 'sklearn',\n",
    "        'matplotlib': 'plt',\n",
    "        'obspy': 'obspy',\n",
    "        'pywt': 'pywt',\n",
    "        'torch': 'torch',\n",
    "        'torchvision': 'torchvision',\n",
    "        'torchaudio': 'torchaudio',\n",
    "    }\n",
    "\n",
    "    for pkg_name, alias in packages.items():\n",
    "        try:\n",
    "            module = importlib.import_module(pkg_name)\n",
    "            print(f\"{pkg_name} version: {module.__version__}\")\n",
    "        except ImportError:\n",
    "            print(f\"{pkg_name} is NOT installed. Installing it now...\")\n",
    "            get_ipython().system('pip install {pkg_name}')\n",
    "            print(f\"{pkg_name} has been installed.\")\n",
    "        except AttributeError:\n",
    "            print(f\"Could not find the version for {pkg_name}\")\n",
    "\n",
    "show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseed_dir_lunar_train = \"./NasaProject/space_apps_2024_seismic_detection/data/lunar/training/data/S12_GradeA\"\n",
    "mseed_dir_lunar_test = \"./NasaProject/space_apps_2024_seismic_detection/data/lunar/test/data/S12_GradeB\"\n",
    "mseed_dir_mars_train = \"./NasaProject/space_apps_2024_seismic_detection/data/mars/training/data\"\n",
    "mseed_dir_mars_test = \"./NasaProject/space_apps_2024_seismic_detection/data/mars/test/data\"\n",
    "\n",
    "csv_path_train = \"train_list.csv\"\n",
    "csv_path_test = \"test_list.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(csv_path, dir_paths):\n",
    "    with open(csv_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['fname'])\n",
    "        \n",
    "        for dir_path in dir_paths:\n",
    "            for root, _, files in os.walk(dir_path):\n",
    "                for filename in files:\n",
    "                    if filename.endswith('.mseed'):\n",
    "                        file_path = os.path.join(root, filename)\n",
    "                        writer.writerow([file_path])\n",
    "\n",
    "append_to_csv(csv_path_train, [mseed_dir_lunar_train, mseed_dir_mars_train])\n",
    "print(f\"Training CSV file has been created at: {csv_path_train}\")\n",
    "\n",
    "append_to_csv(csv_path_test, [mseed_dir_lunar_test, mseed_dir_mars_test])\n",
    "print(f\"Testing CSV file has been created at: {csv_path_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "    return normalized_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1DAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1DAutoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5, stride=2, padding=2),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 16, kernel_size=3, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=3, stride=1, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=3, stride=1, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=3, stride=1, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn1d_autoencoder(train_data, input_size, epochs=30, learning_rate=0.001, batch_size=1024, save_path=\"cnn1d_autoencoder.pth\"):\n",
    "    autoencoder = CNN1DAutoencoder().to(device)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "        autoencoder = torch.nn.DataParallel(autoencoder)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_data = torch.tensor(train_data, dtype=torch.float32).to(device)\n",
    "    print(f\"Loaded entire dataset into GPU memory: {train_data.shape}\")\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    num_batches = train_data.shape[0] // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        autoencoder.train()\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_data = train_data[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = autoencoder(batch_data)\n",
    "            loss = criterion(outputs, batch_data)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            progress = (batch_idx + 1) / num_batches\n",
    "            progress_percentage = progress * 100\n",
    "            progress_bar = f\"[{'#' * int(progress * 40)}{'.' * (40 - int(progress * 40))}]\"\n",
    "            sys.stdout.write(f\"\\rEpoch [{epoch+1}/{epochs}] Progress: {progress_bar} {progress_percentage:.2f}%\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"\\nEpoch [{epoch+1}/{epochs}] completed, Loss: {avg_loss:.4f}, Time per epoch: {epoch_time:.2f} seconds\")\n",
    "\n",
    "        model_save_path = f\"cnn1d_autoencoder_epoch_{epoch+1}_loss_{avg_loss:.4f}.pth\"\n",
    "        torch.save(autoencoder.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_with_scoring(autoencoder, test_data, threshold=0.5, z_thresh=5.85):\n",
    "    test_data = torch.tensor(test_data, dtype=torch.float32).to(device)\n",
    "    autoencoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructions = autoencoder(test_data)\n",
    "        reconstruction_error = np.mean(np.abs(test_data.cpu().numpy() - reconstructions.cpu().numpy()), axis=1)\n",
    "\n",
    "    weighted_error = reconstruction_error * np.where(reconstruction_error > threshold, 2, 1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    weighted_error_normalized = scaler.fit_transform(weighted_error.reshape(-1, 1)).flatten()\n",
    "\n",
    "    z_scores = zscore(weighted_error_normalized)\n",
    "\n",
    "    outliers = np.where(np.abs(z_scores) > z_thresh, -1, 1)\n",
    "\n",
    "    anomaly_scores = weighted_error_normalized\n",
    "\n",
    "    return anomaly_scores, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_with_anomaly_scores(original_data, anomaly_scores, outliers, base_path=\"C:/Users/jacks/Downloads/NasaProject\"):\n",
    "    plots_dir = os.path.join(base_path, \"plots\")\n",
    "    if not os.path.exists(plots_dir):\n",
    "        os.makedirs(plots_dir)\n",
    "\n",
    "    time_array = np.arange(len(original_data))\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(time_array, original_data, label=\"Original Signal\", alpha=0.7)\n",
    "    plt.title(\"Original Signal\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(time_array, anomaly_scores, label=\"Anomaly Scores\", color=\"red\", alpha=0.7)\n",
    "    plt.title(\"Anomaly Scores over Time\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.plot(time_array, original_data, label=\"Original Signal\", alpha=0.7)\n",
    "\n",
    "    anomaly_times = time_array[outliers == -1]\n",
    "    anomaly_severity = anomaly_scores[outliers == -1]\n",
    "\n",
    "    plt.scatter(anomaly_times, original_data[anomaly_times], \n",
    "                c=anomaly_severity, cmap='hot', label=\"Anomalies\", marker='x')\n",
    "\n",
    "    plt.title(\"Anomalies Detected on Original Signal\")\n",
    "    plt.colorbar(label='Anomaly Severity')\n",
    "    plt.legend()\n",
    "\n",
    "    existing_files = os.listdir(plots_dir)\n",
    "    file_index = len(existing_files) + 1\n",
    "    save_path = os.path.join(plots_dir, f\"anomalies_detected_{file_index}.png\")\n",
    "\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_training_data(file_list, num_files_to_load=None):\n",
    "    combined_data = []\n",
    "\n",
    "    if num_files_to_load:\n",
    "        file_list = file_list[:num_files_to_load]\n",
    "\n",
    "    for file_path in file_list:\n",
    "        print(f'Loading {file_path}')\n",
    "        stream = obspy.read(file_path)\n",
    "        trace = stream[0]\n",
    "        data = trace.data\n",
    "\n",
    "        normalized_data, _ = normalize_data(data)\n",
    "\n",
    "        combined_data.append(normalized_data)\n",
    "\n",
    "    train_data = np.vstack(combined_data)\n",
    "    train_data = np.expand_dims(train_data, axis=1)\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mseed_file(file_path, autoencoder):\n",
    "    stream = obspy.read(file_path)\n",
    "    trace = stream[0]\n",
    "    data = trace.data\n",
    "\n",
    "    normalized_data, scaler = normalize_data(data)\n",
    "\n",
    "    normalized_data = np.expand_dims(normalized_data, axis=1)\n",
    "    normalized_data_tensor = torch.tensor(normalized_data, dtype=torch.float32).to(device)\n",
    "\n",
    "    anomaly_scores, outliers = detect_anomalies_with_scoring(autoencoder, normalized_data_tensor)\n",
    "\n",
    "    anomaly_scores_denormalized = scaler.inverse_transform(anomaly_scores.reshape(-1, 1))\n",
    "\n",
    "    min_len = min(len(data), len(anomaly_scores_denormalized))\n",
    "    data = data[:min_len]\n",
    "    anomaly_scores_denormalized = anomaly_scores_denormalized[:min_len]\n",
    "    outliers = outliers[:min_len]\n",
    "\n",
    "    plot_results_with_anomaly_scores(data, anomaly_scores_denormalized, outliers)\n",
    "\n",
    "    return anomaly_scores_denormalized, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anomaly_percentage(outliers):\n",
    "    windows = np.array_split(outliers, 10000)\n",
    "    anomalous_windows = 0\n",
    "\n",
    "    for window in windows:\n",
    "        anomalies_in_window = np.sum(window == -1)\n",
    "        if anomalies_in_window > 1:\n",
    "            anomalous_windows += 1\n",
    "\n",
    "    anomaly_percentage = (anomalous_windows / 1000) * 100\n",
    "    return anomaly_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = r\"train_list.csv\"\n",
    "file_list = pd.read_csv(csv_file)['fname']\n",
    "\n",
    "train_data = load_and_prepare_training_data(file_list, num_files_to_load=77)\n",
    "print(f'Total number of data points in the training data: {train_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = train_cnn1d_autoencoder(train_data, input_size=1, epochs=2, batch_size=65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "csv_file = \"test_list.csv\"\n",
    "file_list = pd.read_csv(csv_file)['fname']\n",
    "\n",
    "test_data = load_and_prepare_training_data(file_list, num_files_to_load=77)\n",
    "print(f'Total number of data points in the test data: {test_data.shape[0]}')\n",
    "\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_anomaly_percentage = 0\n",
    "file_count = len(file_list)\n",
    "\n",
    "for file_path in file_list:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'Processing {file_path}')\n",
    "    \n",
    "    reconstruction_error, outliers = process_mseed_file(file_path, autoencoder)\n",
    "    \n",
    "    anomaly_percentage = calculate_anomaly_percentage(outliers)\n",
    "    total_anomaly_percentage += anomaly_percentage\n",
    "    \n",
    "    print(f'Outliers: {outliers}')\n",
    "    print(f'Reconstruction Error: {reconstruction_error}')\n",
    "    print(f'Anomaly Percentage: {anomaly_percentage:.2f}%')\n",
    "\n",
    "average_anomaly_percentage = total_anomaly_percentage / file_count\n",
    "print(f'Overall Average Anomaly Percentage: {average_anomaly_percentage:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
